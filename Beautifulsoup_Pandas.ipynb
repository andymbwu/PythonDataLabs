{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <h1> <center> ENSF 519.01 Programming Fundamentals for Data Engineers </center></h1>\n",
    "<h2> <center> Assignment 6: Beautifulsoup and Pandas(100 marks)</center></h2>\n",
    "<h3> <center> Due: This Lab will not be graded. The assignment will be solved as an example in class this week.  </center></h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit this file and write your solutions to the problems in sections specified with `# Your solution goes here`. Test your code and when you were done, download this notebook as an `.ipynb` file and submit it to D2L. To get this file, in Jupyter notebook you can go to File -> Download as -> Notebook(.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl and Scrape \n",
    "\n",
    "Shulich wants to have an integrated dataset of all engineering professors in one place. So as a data engineer, you're asked to gather some information of engineering professors by crawling different faculty's website. Then, scrape their information and load them to a pandas dataframe and eventually save it as a csv file. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the first step, you need to get the html text of the website using `requests` library, and then you must use `Beautifulsoup4` library and `lxml` parser to pars the html and extract the needed information.     \n",
    "\n",
    "#### Before You start: \n",
    "* Install __Beautifulsoup4__ library using pip: <br>\n",
    "    `pip install beautifulsoup4`\n",
    "\n",
    "* Install __lxml__ parser using pip: <br>\n",
    "    `pip install lxml`\n",
    "    \n",
    "* Install __requests__ library using pip: <br>\n",
    "    `pip install requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A.(55 pts)\n",
    "\n",
    "In the following, all engineering faculties' websites are listed in the URLs. First, for each faculty, get the html text of the webpage and scrpe the information of all its professors to put them in a dataframe as presented below: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You're DataFrame must be like : \n",
    "\n",
    "|firsname|lastname|title|faculty|email|phone|homePage|\n",
    "|--------|--------|-------------|-----|-----|--------|\n",
    "|        |        |      |     |      |     |        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Use `Inspect Element` of Chrome to see the mapping html tags to objects in a webpage<br>\n",
    "__Tutorial Link__: https://www.youtube.com/watch?v=1l4xz1QQhew&t=3s\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-4-ef7f6605da16>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-ef7f6605da16>\"\u001b[1;36m, line \u001b[1;32m22\u001b[0m\n\u001b[1;33m    for row in soup.find('div,id' = 'uc-content').find_all('tr'):\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "# Your solution goes here\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urlib3.disable\n",
    "\n",
    "URLs=[\"https://schulich.ucalgary.ca/departments/electrical-and-computer-engineering/faculty\",\n",
    "\"https://schulich.ucalgary.ca/departments/chemical-and-petroleum-engineering/faculty\",\n",
    "\"https://schulich.ucalgary.ca/departments/civil-engineering/faculty\",\n",
    "\"https://schulich.ucalgary.ca/departments/mechanical-and-manufacturing-engineering/faculty\",\n",
    "\"https://schulich.ucalgary.ca/departments/geomatics-engineering/faculty\"]\n",
    "\n",
    "html = requests.get(URLs[0], verify = False).text   #verify=false ensures that you don't need to go through site security\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "#need dataframe\n",
    "ProfessorDF = pd.DataFrame(columns = {'firstname','lastname','title'})\n",
    "\n",
    "                                      \n",
    "                                      \n",
    "#goes through faculty list row by row\n",
    "for row in soup.find('div',id = 'uc-content').find_all('tr'):\n",
    "    #not all fields will have all values, so nan is the placeholder\n",
    "    firstname=lastname=title=faculty=email=phone=homePage-np.nan\n",
    "    for i,column in enumerate(row.find_all('td')):\n",
    "        if i == 0:\n",
    "            homePage = \"https:\\\\schlich.ucalgary.ca\"+column.a('href')\n",
    "            name = [part.strip() for part in column.text.split(',')]\n",
    "            firstname=name[i]\n",
    "            lastname=name[0]\n",
    "    ProfessorsDF = ProfessorsDF.append({'firstname':firstname,'lastname':lastname})\n",
    "ProfessorsDF = ProfessorsDF.dropna\n",
    "\n",
    "                                      \n",
    "                                        \n",
    "                   \n",
    "                \n",
    "                     \n",
    "                     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B (45 pts)\n",
    "\n",
    "### part I (30 pts)\n",
    "In this part, iterate on professors dataframe and request to get their homepage html, and find the address of each professor and add it to your previous dataframe as a new feature.  Eventually save the dataframe as a csv file. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> <br>\n",
    "Put `Nan` for some of the professors who do not have address in their webpage\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your solution goes here\n",
    "for i,prof in ProfessorsDF.iterrows():\n",
    "    \n",
    "    source = requests.get(prof['homepage'],verify=false).text\n",
    "    soup = BeautifulSoup(source,'lxml')\n",
    "    \n",
    "    for each in soup.find_all('div',class )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "ProfessorDF.to_csv(\"./Professors.csv\")\n",
    "ProfessorDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part II (15pts)\n",
    "\n",
    "In this part, you need to generate the following reports:\n",
    "  \n",
    "  \n",
    "* Number of Full professors\n",
    "* Number of Associate professors\n",
    "* Number of Assistant professors\n",
    "* Number of Adjunct professors \n",
    "* Number of Instructor professors\n",
    "* Number of Emeritus professors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your solution goes here\n",
    "full_Prof = 0\n",
    "for i,prof in ProfessorsDF['title'].iteritems():\n",
    "    titles =[title.lower() for title in prof.split(',')]\n",
    "    if \"professor\" in titles:\n",
    "        full_Prof += 1\n",
    "\n",
    "print(\"\\t*********** REPORT ***************\")\n",
    "print(\"\\tNumber of Full professors: \",full_Prof)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
